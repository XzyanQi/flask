# -*- coding: utf-8 -*-
"""NLP_Translate.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1a8FIZFsbWpsPfCVair13zMbVV7dgoWQf

# **Capstone Project** (CC25-CF254)

**Mindfulness**

Mindfulness is a chatbot application designed to help college students recognize their emotional codition. This chatbot can ask questions about mood, sleep patterns, stress level, and their daily activities.

## **Import All Packages/Library dan Install package**
"""

# Pasang paket yang dibutuhkan
# pip install PySastrawi  # Pasang PySastrawi untuk stemming bahasa Indonesia
# pip install transformers[torch] faiss-cpu sentencepiece datasets  # Pasang transformers untuk NLP modern
# pip install tensorflow  # Pasang tensorflow untuk deep learning
# pip install nltk  # Pasang nltk untuk natural language toolkit
# pip install sentencepiece  # Pasang sentencepiece untuk tokenisasi
# pip install scikit-learn  # Pasang scikit-learn untuk machine learning
# pip install faiss-cpu

import nltk
import re
import string
import numpy as np
import pandas as pd
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from transformers import AutoTokenizer, TFAutoModel
import faiss
from sklearn.metrics.pairwise import cosine_similarity
import matplotlib.pyplot as plt
import tensorflow as tf

nltk.download('punkt')
nltk.download('punkt_tab')
nltk.download('stopwords')

# Preprocessing function
def preprocess_text(text):
    if isinstance(text, str):
        text = text.lower()
        text = text.translate(str.maketrans('', '', string.punctuation))
        text = re.sub(r'\d+', '', text)
        stop_words = set(stopwords.words('indonesian'))
        words = word_tokenize(text)
        return ' '.join([word for word in words if word not in stop_words])
    return ''

# Load dataset
data = pd.read_csv('translated_train.csv')
data['translated_response'].fillna("Sorry, I have no answer for this.", inplace=True)
data['processed_context'] = data['translated_context'].apply(preprocess_text)
data['processed_response'] = data['translated_response'].apply(preprocess_text)
data = data.drop_duplicates(subset=['processed_context', 'processed_response']).reset_index(drop=True)
df_terjemahan = data.copy()

# Load IndoBERT Tokenizer & Model (hanya TensorFlow)
model_name = "cahya/distilbert-base-indonesian"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = TFAutoModel.from_pretrained(model_name)

# Encode function

def encode_text(text):
    clean_text = preprocess_text(text)
    inputs = tokenizer(clean_text, return_tensors='tf', padding=True, truncation=True, max_length=128)
    outputs = model(inputs)
    embeddings = tf.reduce_mean(outputs.last_hidden_state, axis=1)
    return embeddings.numpy().squeeze()

# Encode corpus
corpus_embeddings = np.array([encode_text(t) for t in df_terjemahan['processed_context']])
np.save('corpus_embeddings.npy', corpus_embeddings)
corpus_embeddings = np.load('corpus_embeddings.npy')

# FAISS index
index = faiss.IndexFlatL2(corpus_embeddings.shape[1])
index.add(corpus_embeddings)

# Query embedding

def get_query_embedding(text):
    clean_text = preprocess_text(text)
    inputs = tokenizer(clean_text, return_tensors='tf', padding=True, truncation=True, max_length=128)
    outputs = model(inputs)
    pooled = tf.reduce_mean(outputs.last_hidden_state, axis=1)
    return pooled.numpy().reshape(1, -1)

# Chatbot response with fallback

def get_semantic_chatbot_response_with_fallback(query, df, index, similarity_threshold=1.0):
    query_emb = get_query_embedding(query)
    distances, indices = index.search(query_emb.astype('float32'), k=1)
    closest_distance = distances[0][0]
    closest_idx = indices[0][0]

    if closest_distance <= similarity_threshold:
        return df.iloc[closest_idx]['translated_response']
    else:
        return "Maaf, aku kurang memahami maksudmu. Bisa kamu jelaskan lebih lanjut?"

# Uji coba
print("\nUji Coba Mindfulness\n")
queries = [
    "Saya merasa sangat sedih.",
    "Butuh bantuan untuk mengatasi kecemasan",
    "Bagaimana seseorang memulai proses konseling?"
]
SIMILARITY_THRESHOLD = 50.0

for i, query in enumerate(queries, start=1):
    print(f"Query Pengguna {i}: {query}")
    response = get_semantic_chatbot_response_with_fallback(query, df_terjemahan, index, similarity_threshold=SIMILARITY_THRESHOLD)
    print(f"Mindfulness {i}: {response}\n")

# Visualisasi distribusi similarity
print("Visualisasi distribusi similarity...")
dist_list = []
for q in df_terjemahan['processed_context'].sample(50):
    q_emb = get_query_embedding(q)
    d, _ = index.search(q_emb.astype('float32'), k=1)
    dist_list.append(d[0][0])

plt.hist(dist_list, bins=20)
plt.title("Distribusi Nilai Similarity (Distance) di FAISS")
plt.xlabel("Jarak")
plt.ylabel("Jumlah")
plt.show()
